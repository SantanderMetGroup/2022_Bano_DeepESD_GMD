{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downscaling Multi-Model Climate Projection Ensembles with Deep Learning (DeepESD): Contribution to CORDEX EUR-44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Geoscientific Model Development***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**J. Baño-Medina, R. Manzanas, E. Cimadevilla, J. Fernández, J. González-Abad, A.S. Cofiño, and J.M. Gutiérrez**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reproduces the results presented in **Downscaling Multi-Model Climate Projection Ensembles with Deep Learning (DeepESD): Contribution to CORDEX EUR-44**, submitted to *Geoscientific Model Development* by *J. Baño-Medina, R. Manzanas, E. Cimadevilla, J. Fernández, J. González-Abad, A.S. Cofiño and J.M. Gutiérrez*. \n",
    "This paper presents *DeepESD*, the first dataset of high-resolution (0.5º) climate change projections (up to 2100) of daily precipitation and temperature over Europe obtained with deep learning techniques (in particular convolutional neural networks) from an ensemble of eight global climate models from the Coupled Model Intercomparison Project version 5 (CMIP5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparing-the-R-environment-and-working-directories\" data-toc-modified-id=\"Preparing-the-R-environment-and-working-directories-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparing the R environment and working directories</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convolutional-Neural-Networks-(CNNs)\" data-toc-modified-id=\"Convolutional-Neural-Networks-(CNNs)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Convolutional Neural Networks (CNNs)</a></span></li></ul></li><li><span><a href=\"#DeepESD\" data-toc-modified-id=\"DeepESD-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>DeepESD</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preparing-the-predictor-datasets\" data-toc-modified-id=\"Preparing-the-predictor-datasets-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Preparing the predictor datasets</a></span></li><li><span><a href=\"#Precipitation-downscaling\" data-toc-modified-id=\"Precipitation-downscaling-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Precipitation downscaling</a></span></li><li><span><a href=\"#Temperature-downscaling\" data-toc-modified-id=\"Temperature-downscaling-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Temperature downscaling</a></span></li></ul></li><li><span><a href=\"#Dynamical-climate-models\" data-toc-modified-id=\"Dynamical-climate-models-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Dynamical climate models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ensemble-of-Global-Climate-Models-(GCMs)\" data-toc-modified-id=\"Ensemble-of-Global-Climate-Models-(GCMs)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Ensemble of Global Climate Models (GCMs)</a></span></li><li><span><a href=\"#Ensemble-of-Regional-Climate-Models-(RCMs)\" data-toc-modified-id=\"Ensemble-of-Regional-Climate-Models-(RCMs)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Ensemble of Regional Climate Models (RCMs)</a></span></li></ul></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ensemble-mean-and-bias-with-respect-to-E-OBS\" data-toc-modified-id=\"Ensemble-mean-and-bias-with-respect-to-E-OBS-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Ensemble mean and bias with respect to E-OBS</a></span></li><li><span><a href=\"#Climate-change-signals\" data-toc-modified-id=\"Climate-change-signals-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Climate change signals</a></span></li><li><span><a href=\"#Time-series\" data-toc-modified-id=\"Time-series-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Time-series</a></span></li></ul></li><li><span><a href=\"#Technical-specifications\" data-toc-modified-id=\"Technical-specifications-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Technical specifications</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[next notebook](2022_Bano_DeepESD_GMD.ipynb)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook was run on a machine with the following technical specifications:\n",
    "\n",
    "- Operating system: Ubuntu 18.04.3 LTS (64 bits)\n",
    "- Memory: 60 GiB\n",
    "- Processor: 2x Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz (16 cores, 32 threads)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the R environment and working directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is written in the free programming language `R` (version 3.6.1) and builds on [`climate4R`](https://doi.org/10.1016/j.envsoft.2018.09.009) (hereafter C4R), a suite of `R` packages developed by the Santander Meteorology Group for transparent climate data access, post processing (including bias correction and downscaling) and visualization. For details on C4R, the interested reader is referred to [Iturbide et al. 2019](https://doi.org/10.1016/j.envsoft.2018.09.009)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the following C4R libraries are used along the notebook:\n",
    "\n",
    "\n",
    " * `loadeR` (v1.7.0) for data loading,\n",
    " * `loadeR.2nc` (v0.1.1) for data loading, \n",
    " * `transformeR` (v2.1.0) for data manipulation, \n",
    " * [`downscaleR`](https://doi.org/10.5194/gmd-13-1711-2020) (v3.3.2) for downscaling and\n",
    " * [`downscaleR.keras`](https://doi.org/10.5194/gmd-13-2109-2020) (v1.0.0) for downscaling with neural networks and\n",
    " * `visualizeR` (v1.6.0) data visualization\n",
    " * `climate4R.UDG` (v0.2.3) datasets collection\n",
    " * `climate4R.value` (v0.0.2) VALUE indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A frozen version of the above libraries and all the ones needed to reproduce the manuscript are installable through the `environment.yml` file using the `mamba` package, by executing the following commands in your command shell terminal: \n",
    "```shell\n",
    "$ mamba env create -n deepesd --file=environment.yml\n",
    "```\n",
    "\n",
    "Once installed, you activate the newly created `deepesd` environment using `conda` by typing the following command:\n",
    "```shell\n",
    "$ conda activate deepesd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T21:10:13.881533Z",
     "start_time": "2022-05-19T21:10:11.793Z"
    }
   },
   "outputs": [],
   "source": [
    "options(java.parameters = \"-Xmx8g\")  # expanding Java memory\n",
    "\n",
    "# C4R libraries\n",
    "library(loadeR)\n",
    "library(loadeR.2nc)\n",
    "library(transformeR)\n",
    "library(downscaleR)\n",
    "library(downscaleR.keras) \n",
    "library(visualizeR)\n",
    "library(climate4R.value)\n",
    "\n",
    "# Other useful libraries\n",
    "library(magrittr)  # to pipe commands using '%>%' or '%<>%'\n",
    "\n",
    "# For visualization purposes\n",
    "library(RColorBrewer)\n",
    "library(gridExtra)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate data, predictions and models, generated along this notebook, are saved in a tree of directories created on the notebook's working directory. Please use the `dir.create` function to create two new directories (`data` and `models`) in your working directory. \n",
    "Within each of these directories, create subsequently two more subdirectories, named `tas` (near-surface temperature) and `pr` (precipitation) representing the respective predictands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T21:40:04.306134Z",
     "start_time": "2022-05-19T21:40:04.292Z"
    }
   },
   "outputs": [],
   "source": [
    "if (!dir.exists(\"./data/pr\")) dir.create(\"./data/pr\", recursive = TRUE, showWarnings = FALSE)\n",
    "if (!dir.exists(\"./data/tas\")) dir.create(\"./data/tas\", showWarnings = FALSE)\n",
    "if (!dir.exists(\"./models/pr\")) dir.create(\"./models/pr\", recursive = TRUE, showWarnings = FALSE)\n",
    "if (!dir.exists(\"./models/tas\")) dir.create(\"./models/tas\", showWarnings = FALSE)\n",
    "if (!dir.exists(\"./figures\")) dir.create(\"./figures\", showWarnings = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define here the **predictand** that will be used in the rest of the notebook. Predictor variables are defined in the next section, regardless of the predictand selected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define the predictand as 'pr' or 'tas' ---\n",
    "predictand <- 'pr' \n",
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define whether the data files are to be compressed in `gz` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress <- TRUE\n",
    "extension <- if (isTRUE(compress)) {\n",
    "   \"rds.gz\"\n",
    "} else {\n",
    "  \"rds\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and regrid data\n",
    "\n",
    "We are now ready to load into our `R` environment all the data we are going to work with. We provide two different ways to do so:\n",
    "1. Access through the [Climate Data Service](http://meteo.unican.es/cds) developed by the [Santander Met Group](http://meteo.unican.es) (non registered users need to register first [here](http://meteo.unican.es/udg-tap/signup)). Use the `loginUDG` function to log into the service with your own credentials. **See sections 2.1 and 2.2.**\n",
    "2. Point to the data provided in Zenodo. **See section 2.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing the ERA-Interim predictor and E-OBS predictand datasets (optional)\n",
    "***NOTE: Please skip this section if you work only with the Zenodo files and move directly to section 2.3***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how to download the ERA-Interim and E-OBS data used for calibrating the Convolutional Neural Networks (CNN) from UDG. First, we call function `loginUDG` with our user and password to grant access to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Register here: http://meteo.unican.es/udg-tap/signup\n",
    "## and then type :\n",
    "\n",
    "# loginUDG(\"user\",\"password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store in `vars` the name of the predictor variables considered in this study as defined by the C4R vocabulary (type `C4R.vocabulary()` for more info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T08:56:02.721148Z",
     "start_time": "2022-05-21T08:56:02.710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predictor variables considered\n",
    "vars  <- c(\n",
    "  \"z@500\",\"z@700\",\"z@850\",       # Geopotential height at isobaric levels\n",
    "  \"hus@500\",\"hus@700\",\"hus@850\", # Specific humidity at isobaric levels\n",
    "  \"ta@500\",\"ta@700\",\"ta@850\",    # Air temperature at isobaric levels\n",
    "  \"ua@500\",\"ua@700\",\"ua@850\",    # East-ward wind at isobaric levels \n",
    "  \"va@500\",\"va@700\",\"va@850\"     # North-ward wind at isobaric levels \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code allows for loading the ERA-Interim predictor variables, which are needed to train our neural networks, for the period 1979-2005 by using the `loadGridData` function. Subsequently, the `makeMultiGrid` creates a unique C4R object containing all this information. \n",
    "\n",
    "Moreover, since these loading steps can be quite time-consuming, we save the predictors `x` and predictand `y` data into R objects, which are also provided for reproducibility of the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output.file = sprintf(\"./data/x_ERA-Interim.%s\", extension) \n",
    "if (! file.exists(output.file)){\n",
    "  x <- lapply(vars, function(var) {\n",
    "    loadGridData(\n",
    "      dataset = \"ECMWF_ERA-Interim-ESD\",\n",
    "      var     = var,\n",
    "      lonLim  = c(-8,34), latLim  = c(34,76),  # domain of the predictors\n",
    "      years   = 1979:2005\n",
    "    )}\n",
    "  ) %>% makeMultiGrid()\n",
    "  saveRDS(x, output.file, compress = compress)\n",
    "} else {\n",
    "  x <- readRDS(sprintf(\"./data/x_ERA-Interim.%s\", extension))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predictands we use temperature and precipitation from E-OBS, which can be obtained as netCDF files [here](https://www.ecad.eu/download/ensembles/download.php).\n",
    "Once downloaded, these data can be imported in `R` with the `loadGridData` function.\n",
    "Subsequently, using `transformeR::interpGrid`, we upscale these E-OBS fields from their native 0.25º to the 0.5º regular grid our projections are delivered.\n",
    "We illustrate next how to load either precipitation or temperature with `loadGridData`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve E-OBS data if not available locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eobs.varname <- switch(predictand, pr = \"rr\", tas = \"tg\")\n",
    "eobs.data <- sprintf(\"./data/%s/%s_ens_mean_0.25deg_reg_v20.0e.nc\", predictand, eobs.varname)\n",
    "if (! file.exists(eobs.data)){\n",
    "  download.file(\n",
    "    sprintf(\"https://knmi-ecad-assets-prd.s3.amazonaws.com/ensembles/data/Grid_0.25deg_reg_ensemble/%s_ens_mean_0.25deg_reg_v20.0e.nc\", eobs.varname),\n",
    "    eobs.data\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upscale E-OBS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T20:34:11.212241Z",
     "start_time": "2022-05-19T20:34:11.195Z"
    }
   },
   "outputs": [],
   "source": [
    "# boundaries of our projections domain and target resolution\n",
    "grid05 = list(\"x\" = c(-9.75,30.25),\"y\" = c(34.25,74.25))\n",
    "attr(grid05,\"resX\") <- attr(grid05,\"resY\") <- 0.5 \n",
    "\n",
    "# Load the predictand (y) and save to file\n",
    "output.file = sprintf(\"./data/%s/y.%s\", predictand, extension)\n",
    "if (! file.exists(output.file)){\n",
    "  y <- loadGridData(\n",
    "    dataset = eobs.data,\n",
    "    var = predictand,\n",
    "    lonLim = c(-10,30),\n",
    "    latLim = c(34,75),\n",
    "    years = 1979:2005,\n",
    "    dictionary = \"./inst/dictionaries/E-OBS_v20e.dic\"  \n",
    "  ) %>% interpGrid(new.coordinates = grid05, method = \"bilinear\")\n",
    "  saveRDS(y, output.file, compress = compress)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing the GCM predictor datasets (optional)\n",
    "***NOTE: Please skip this section if you work only with the Zenodo files and move directly to section 2.3***\n",
    "\n",
    "In this section we 1) load the predictor variables of interest from the 8 GCM considered in the study from the Santander CDS, 2) bias-correct and standardize these predictor fields, 3) save these processed fields in `rds` objects to avoid repeating these steps in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following labels identify the 8 GCMs considered in this work in the Santander CDS, for the historical and RCP.8.5 scenario, respectively.\n",
    "These labels are used to build the dataset names when calling the `loadGridData` function for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T21:10:35.796755Z",
     "start_time": "2022-05-19T21:10:35.787Z"
    }
   },
   "outputs": [],
   "source": [
    "## Use UDG.datasets()$CMIP5_subset to obtain the labels of the desired GCMs\n",
    "model.names <- c(\n",
    "  \"CanESM2_r1i1p1\",\n",
    "  \"CNRM-CM5_r1i1p1\",\n",
    "  \"MPI-ESM-MR_r1i1p1\",\n",
    "  \"MPI-ESM-LR_r1i1p1\",\n",
    "  \"NorESM1-M_r1i1p1\",\n",
    "  \"GFDL-ESM2M_r1i1p1\",\n",
    "  \"EC-EARTH_r12i1p1\",\n",
    "  \"IPSL-CM5A-MR_r1i1p1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store in `periods` the historical and future temporal intervals. We also define `fill850na` function which allows to fill the NA values appearing in the 850hPa at certain gridpoints with numeric values of their closest neighbours. Note that this is essential since CNNs can not deal with NA samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill850na <- function(x){\n",
    "  # Since the IPSL contains NA values in the 850hPa level at certain gridpoints,\n",
    "  # we replace these NA values with the numeric values of their closest neighbours.\n",
    "  ind850 <- grepl(\"@850\",x$Variable$varName,fixed = TRUE) %>% which()\n",
    "  indGP <- apply(x$Data[ind850[1],1,,,], MARGIN = c(2,3), anyNA) %>% which(arr.ind = TRUE)\n",
    "  for (i in 1:nrow(indGP)) {\n",
    "    indTime <- is.na(x$Data[ind850[1],1,,indGP[i,1],indGP[i,2]]) %>% which()\n",
    "    x$Data[ind850,1,indTime,indGP[i,1],indGP[i,2]] <- x$Data[ind850,1,indTime,indGP[i,1],indGP[i,2]-1]\n",
    "  }\n",
    "  return(x)\n",
    "}\n",
    "\n",
    "periods <- list(\n",
    "  hist = 1975:2005, # hist: historical\n",
    "  nearf = 2006:2040, # nearf: near-future\n",
    "  midf = 2041:2070, # midf: mid-future\n",
    "  farf = 2071:2100 # farf: far-future\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop allows us to load the predictors from the above GCMs over our target domain for the reference (1975-2005) and future (early-future: 2006-2040, mid-future: 2041-2070, far-future: 2071-2100) periods of interest. Note that the historical experiment is used for the reference period and the RCP8.5 scenario for the future periods. Note also that, within the loop, all GCMs are interpolated to the spatial resolution of the ERA-Interim predictors which were used to fit the CNNs. Once loaded, the GCM predictors are saved as `.rds` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (period in names(periods)) {\n",
    "  for (model.name in model.names) {\n",
    "    output.file <- sprintf(\"./data/x_%s_%s.%s\",\n",
    "                           period,\n",
    "                           gsub(\"_.*\",\"\",model.name),\n",
    "                           extension\n",
    "    )\n",
    "    if (! file.exists(output.file)){\n",
    "      if (period == \"hist\"){\n",
    "        dataset.name <- sprintf(\"CMIP5-subset_%s_historical\", model.name)\n",
    "      } else {\n",
    "        dataset.name <- sprintf(\"CMIP5-subset_%s_rcp85\", model.name)\n",
    "      }\n",
    "      x.gcm <- lapply(vars, function(var) {\n",
    "        loadGridData(\n",
    "          dataset = dataset.name,\n",
    "          var = var,\n",
    "          lonLim = c(-8,34), latLim = c(34,76),\n",
    "          years = periods[[period]]\n",
    "        ) %>% interpGrid(new.coordinates = getGrid(x))\n",
    "      }) %>% makeMultiGrid()\n",
    "      \n",
    "      if (substr(model.name,1,4) == \"IPSL\") {\n",
    "        x.gcm <- fill850na(x.gcm)\n",
    "      }\n",
    "      \n",
    "      saveRDS(x.gcm, file = output.file, compress = compress)\n",
    "      # Free memory\n",
    "      gc()\n",
    "    }\n",
    "  }  \n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop allows us to bias-correct and standardize the GCM predictors loaded in the previous step to assure they reasonably resemble the ERA-Interim variables used to train the CNN models (note this is one of the key assumptions that are done in ''perfect-prognosis'' downscaling). The bias-correction of the predictors is defined in the `scalingDeltaMapping` function leaning on the `scaleGrid` function from `transformeR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ref.period <- 1979:2005\n",
    "\n",
    "scalingDeltaMapping <- function(grid, base, ref) {\n",
    "  ### remove the seasonal trend\n",
    "  grid_detrended <- scaleGrid(grid, \n",
    "                              base = grid, \n",
    "                              ref = base, \n",
    "                              type = \"center\", \n",
    "                              spatial.frame = \"gridbox\", \n",
    "                              time.frame = \"monthly\")  \n",
    "  \n",
    "  ### bias correct the mean and variance\n",
    "  grid_detrended_corrected <- scaleGrid(grid_detrended, \n",
    "                                        base = base, \n",
    "                                        ref = ref, \n",
    "                                        type = \"standardize\", \n",
    "                                        spatial.frame = \"gridbox\", \n",
    "                                        time.frame = \"monthly\")    \n",
    "  \n",
    "  ### add the seasonal trend\n",
    "  grid_corrected <- scaleGrid(grid_detrended_corrected, \n",
    "                              base = base, \n",
    "                              ref = grid, \n",
    "                              type = \"center\", \n",
    "                              spatial.frame = \"gridbox\", \n",
    "                              time.frame = \"monthly\")    \n",
    "  \n",
    "  ### return \n",
    "  return(grid_corrected)\n",
    "}\n",
    "\n",
    "standardize <- function(grid, base){\n",
    "  scaleGrid(grid, \n",
    "    base = base,\n",
    "    ref = NULL,\n",
    "    type = \"standardize\"\n",
    "  ) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over the global climate models and scenarios performing the biascorrection+standardization procedure. This is done by calling the `scalingDeltaMapping` function defined above and then scaling based on the ERA-Interim mean and standard deviation fields. The so-processed GCM predictors, wich will be used as inputs to the CNN models, are saved as `rds` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for (model.name in model.names) {\n",
    "  model.basename <- gsub(\"_.*\",\"\",model.name)\n",
    "  for (period in names(periods)) {\n",
    "    output.file <- sprintf(\"./data/xn_%s_%s.%s\", \n",
    "                           period, \n",
    "                           gsub(\"_.*\",\"\",model.name),\n",
    "                           extension)\n",
    "    if (! file.exists(output.file)){\n",
    "      raw.x <- readRDS(sprintf(\"./data/x_%s_%s.%s\", period, model.basename, extension))\n",
    "      if (period == \"hist\") {\n",
    "        harmonize.base <- subsetGrid(raw.x, years = ref.period)\n",
    "      } \n",
    "      x.harm <- scalingDeltaMapping(raw.x, base = harmonize.base, ref = x)\n",
    "      xn <- standardize(x.harm, base = x)\n",
    "      \n",
    "      # Save the standardized predictor fields as `rds` objects  \n",
    "      saveRDS(xn,\n",
    "              file = output.file,\n",
    "              compress = compress\n",
    "      )\n",
    "      rm(xn)\n",
    "      # Free memory\n",
    "      gc()\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load data from Zenodo\n",
    "First you need to set the working directory to the path containing the Zenodo files. These files can be found in ***!!! zenodo doi !!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setwd(\"path_to_the_zenodo_data_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, users will find the output products derived from sections 2.1 and 2.2. In the next chunk of code we load the predictor (`x`) and predictand (`y`) datasets used for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- readRDS(sprintf(\"./data/x_ERA-Interim.%s\", extension))\n",
    "y <- readRDS(sprintf(\"./data/%s/y.%s\", predictand, extension))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep-ESD\n",
    "This section shows how to fit the CNN model which links the large-scale predictors from ERA-Interim with the high-resolution E-OBS precipitation or temperature. The steps would be:\n",
    "- Prepare the predictor and predictand tensors with the `prepareData.keras` function from `downscaleR.keras`.\n",
    "- Standardize the ERA-Interim predictors using `trasformeR::scaleGrid`. \n",
    "- For precipitation, for a better fit of the Gamma distribution, 0.99 is substracted from observed precipitation and negative values are ignored (note that this step implies that rainy days are defined as those receiving 1 or more mm of precipitation). To do this, the `gridArithmetics` and `binaryGrid` functions from `transformeR` are used.\n",
    "- Train the CNN model encapsulaled in the `modelCNN` function with the `downscaleTrain.keras` function from `downscaleR.keras`. To optimize the negative log-likelihood of the Bernoulli-Gamma distribution, we employ the custom loss function `bernouilliGammaLoss` from `downscaleR.keras`. The network is fitted using the Adam optimizer and a learning rate of 1e-4. Early-stopping with a patience of 30 epochs is applied and the best model (epoch) is saved in the working directory as a `.h5` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 1 Convolutional neural network topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build *DeepESD* we rely on the CNNs presented in [Baño-Medina et al. 2020](https://gmd.copernicus.org/articles/13/2109/2020/); in particular, on the CNN1 and CNN10 models, which were found to provide robust results for precipitation and temperature, respectively, both in ''perfect-prognosis'' conditions but also in the GCM space ([Baño-Medina et al. 2020](https://doi.org/10.1007/s00382-021-05847-0)). The cell below shows how to build these CNN models based on `Keras`, and save them in a custom function called `modelCNN`. Note that precipitation and temperature CNN models are different. \n",
    "We refer the reader to [Baño-Medina et al. 2020](https://gmd.copernicus.org/articles/13/2109/2020/) for further details about the exact configuration of the CNNs used herein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T20:48:35.179232Z",
     "start_time": "2022-05-19T20:48:35.171Z"
    }
   },
   "outputs": [],
   "source": [
    "modelCNN <- function(inp) {\n",
    "  padding = switch(predictand, pr = \"same\", tas = \"valid\")\n",
    "  filters.l1 = 50\n",
    "  filters.l2 = 25\n",
    "  filters.l3 = switch(predictand, pr = 1, tas = 10)\n",
    "  # Input layer\n",
    "  inputs <- layer_input(shape = dim(inp$x.global)[2:4])\n",
    "  # Hidden layers\n",
    "  l1 = layer_conv_2d(inputs, filters = filters.l1, kernel_size = c(3,3), activation = 'relu', padding = padding)\n",
    "  l2 = layer_conv_2d(    l1, filters = filters.l2, kernel_size = c(3,3), activation = 'relu', padding = padding)\n",
    "  l3 = layer_conv_2d(    l2, filters = filters.l3, kernel_size = c(3,3), activation = 'relu', padding = padding)\n",
    "  l4 = layer_flatten(    l3)\n",
    "  # Output layer (depends on predictand)\n",
    "  if (predictand == \"pr\") {\n",
    "    l51 = layer_dense(l4, units = dim(inp$y$Data)[2], activation = 'sigmoid') \n",
    "    l52 = layer_dense(l4, units = dim(inp$y$Data)[2], activation = 'linear' ) \n",
    "    l53 = layer_dense(l4, units = dim(inp$y$Data)[2], activation = 'linear' ) \n",
    "    outputs <- layer_concatenate(list(l51,l52,l53))\n",
    "  } else if (predictand == \"tas\") {\n",
    "    l51 = layer_dense(l4, units = dim(inp$y$Data)[2], activation = 'linear') \n",
    "    l52 = layer_dense(l4, units = dim(inp$y$Data)[2], activation = 'linear') \n",
    "    outputs <- layer_concatenate(list(l51, l52)) \n",
    "  }\n",
    "  model <- keras_model(inputs = inputs, outputs = outputs) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Downscaling with Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize them depending on the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T22:08:33.170840Z",
     "start_time": "2022-05-19T22:08:08.578Z"
    }
   },
   "outputs": [],
   "source": [
    "x <- scaleGrid(x,type = \"standardize\")\n",
    "if (predictand == \"pr\") {\n",
    "  y <- binaryGrid(\n",
    "    gridArithmetics(y, 0.99, operator = \"-\"),\n",
    "    condition = \"GE\",\n",
    "    threshold = 0,\n",
    "    partial = TRUE\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T22:09:53.354271Z",
     "start_time": "2022-05-19T22:09:50.559Z"
    }
   },
   "source": [
    "Prepare predictor and predictand data for downscaling with `downscaleR.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T22:09:53.354271Z",
     "start_time": "2022-05-19T22:09:50.559Z"
    }
   },
   "outputs": [],
   "source": [
    "xy.train <- prepareData.keras(\n",
    "  x = x,\n",
    "  y = y,\n",
    "  first.connection = \"conv\",\n",
    "  last.connection = \"dense\",\n",
    "  channels = \"last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "Running the next cell takes about 1 hour\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T22:25:23.039623Z",
     "start_time": "2022-05-19T22:25:23.029Z"
    }
   },
   "outputs": [],
   "source": [
    "cnnmodel.name <- switch(predictand,\n",
    "  pr=\"CNN1\",\n",
    "  tas=\"CNN10\"\n",
    ")\n",
    "cnnloss <- switch(predictand,\n",
    "  pr = bernouilliGammaLoss(last.connection = \"dense\"),\n",
    "  tas = gaussianLoss(last.connection = \"dense\")\n",
    ")\n",
    "\n",
    "output.file <- sprintf('./models/%s/%s.h5', predictand, cnnmodel.name)\n",
    "if (! file.exists(output.file)) {\n",
    "\n",
    "  # Training the CNN model\n",
    "  downscaleTrain.keras(\n",
    "    obj = xy.train, \n",
    "    model = modelCNN(xy.train),\n",
    "    clear.session = TRUE,\n",
    "    compile.args = \n",
    "      list(\n",
    "        \"loss\" = cnnloss,\n",
    "        \"optimizer\" = optimizer_adam(lr = 0.0001)\n",
    "      ),\n",
    "    fit.args =\n",
    "      list(\n",
    "        \"batch_size\" = 100,\n",
    "        \"epochs\" = 10000,\n",
    "        \"validation_split\" = 0.1,\n",
    "        \"verbose\" = 1,\n",
    "        \"callbacks\" = list(\n",
    "           callback_early_stopping(patience = 30),\n",
    "           callback_model_checkpoint(\n",
    "             filepath = output.file,\n",
    "             monitor = 'val_loss',\n",
    "             save_best_only = TRUE\n",
    "           )\n",
    "        )\n",
    "     )\n",
    "  )\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we use it to predict in both the train (training period using ERA-Interim variables) and the GCM spaces. As per the former, for precipitation, we are interested in the estimation of the parameter `p` (probability of rain), since it is needed later to adjust the frequency of rain in the high-resolution projections obtained from the GCM (see the manuscript for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute `p` in the train period the following is done:\n",
    "- Prepare the predictors which will serve as inputs for the CNN model with the `prepareNewData.keras` function. Subsequently, use them to predict in the train set with the `downscalePredict.keras` function. The `model` argument indicates the path where the CNN model was previously stored, and `C4R.template` is a C4R object used as template for the predictions which provides the proper metadata. Since `downscalePredict.keras` outputs 3 parameters (the probability of rain, `p`, and the logarithmic of the shape and scale parameters of the Gamma distribution, `log_alpha` and `log_beta`), the `subsetGrid` is applied in order to keep only `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T22:27:10.039623Z",
     "start_time": "2022-05-19T22:27:06.438Z"
    }
   },
   "outputs": [],
   "source": [
    "cnnloss.name <- switch(predictand,\n",
    "  pr = \"bernouilliGammaLoss\",\n",
    "  tas = \"gaussianLoss\"\n",
    ")\n",
    "\n",
    "cnn.predict <- function(newdata){\n",
    "  downscalePredict.keras(\n",
    "    newdata = newdata,\n",
    "    C4R.template = y,\n",
    "    clear.session = TRUE,\n",
    "    loss = cnnloss.name,\n",
    "    model = list(\n",
    "      \"filepath\" = sprintf('./models/%s/%s.h5', predictand, cnnmodel.name),\n",
    "      \"custom_objects\" = c(\n",
    "          \"custom_loss\" = cnnloss\n",
    "      )\n",
    "    )\n",
    "  )  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T22:27:10.039623Z",
     "start_time": "2022-05-19T22:27:06.438Z"
    }
   },
   "outputs": [],
   "source": [
    "if (predictand == \"pr\"){\n",
    "  xy.test <- prepareNewData.keras(x, xy.train)\n",
    "  pred_ocu_train <- cnn.predict(xy.test) %>% subsetGrid(var = \"p\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the trained CNN model is used to generate the high-resolution projections building on the 8 GCMs considered in this work. To do so, we perform a loop over the GCMs in which the corresponding predictors (which had been previously saved) are loaded and conveniently transformed using the `prepareNewData.keras` function. Finally, the `log_alpha`, `log_beta` and `p` parameters, which are obtained with the `downscalePredict.keras` function are saved in the `pred` object.\n",
    "- On the one hand, `log_alpha` and `log_beta` are used to obtain the rainfall amount with the `computeRainfall` function from `downscaleR.keras`. The argument `simulate` allows us for specifying if either a stochastic or a deterministic outcome is wanted. The argument `bias` is used to re-center the Gamma distribution to 1mm/day. \n",
    "- On the other hand, we use the `p` parameter to derive the binary event occurrence/non occurrence through the `bynaryGrid` function.\n",
    "- Finally, both series (binary and continuous) are multiplied to produce the complete precipitation time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block of code we define commodity functions to compute the (deterministic) frequency and (stochastic) amount of rain, or the deterministic temperature value (as the mean of the predicted distribution). Then we iterate to predict (downscale) for all models, which are saved in `.nc` format with the `grid2nc` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.pr <- function(pred.obj){\n",
    "  # Deterministic frequency and stochastic amount of rain\n",
    "  computeRainfall(\n",
    "    log_alpha = subsetGrid(pred.obj, var = \"log_alpha\"),\n",
    "    log_beta  = subsetGrid(pred.obj, var = \"log_beta\"),\n",
    "    bias = 1,\n",
    "    simulate = TRUE\n",
    "  ) %>% gridArithmetics(\n",
    "    binaryGrid(\n",
    "      subsetGrid(pred.obj, var = \"p\"),\n",
    "      ref.obs = binaryGrid(y, threshold = 1, condition = \"GE\"),\n",
    "      ref.pred = pred_ocu_train\n",
    "    )\n",
    "  )  \n",
    "}\n",
    "\n",
    "predict.tas <- function(pred.obj){\n",
    "  ## Deterministic version \n",
    "  rval <- subsetGrid(pred.obj, var = \"mean\")\n",
    "  rval$Variable$varName <- \"tas\"\n",
    "  return(rval)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "Running the next cell takes hours\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (model.name in model.names) {\n",
    "  model.basename <- gsub(\"_.*\",\"\",model.name)    \n",
    "  for (period in names(periods)) {\n",
    "    output.file <- sprintf(\"./data/%s/y_CNN_%s_%s.nc4\", \n",
    "                           predictand,\n",
    "                           period, \n",
    "                           model.basename)\n",
    "    if (! file.exists(output.file)) {\n",
    "    xn <- readRDS(sprintf(\"./data/xn_%s_%s.%s\", period, model.basename, extension))\n",
    "    xy.test <- prepareNewData.keras(xn, xy.train)\n",
    "    pred <- cnn.predict(xy.test)\n",
    "    pred.params <- switch(predictand,\n",
    "                          pr = predict.pr(pred),\n",
    "                          tas = predict.tas(pred)\n",
    "    ) \n",
    "    grid2nc(pred.params, NetCDFOutFile = output.file)   \n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamical climate models (optional)\n",
    "***NOTE: Please skip this section if you work only with the Zenodo files and move directly to section 5.***\n",
    "\n",
    "To assess the credibility of DeepESD, it is compared against two different ensembles of dynamical models, the first/second of them formed by Global/Regional Climate Models (GCMs/RCMs). Since DeepESD covers only land, we start by creating a 0.5º land-sea mask which will be later applied to eliminate sea points from both GCMs and RCMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.file <- \"./data/mask.nc4\"\n",
    "if (! file.exists(output.file)){\n",
    "\n",
    "  mask <- gridArithmetics(subsetGrid(y, year = 1990), 0) %>% gridArithmetics(1, operator = \"+\") %>% climatology()\n",
    "  mask$Variable$varName <- \"sftlf\"\n",
    "  grid2nc(mask, NetCDFOutFile = output.file)\n",
    "\n",
    "} else{\n",
    "    \n",
    "  mask <- loadGridData(output.file, var = \"sftlf\")\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ensemble of Global Climate Models (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of eight GCMs are interpolated to our target 0.5º resolution (E-OBS grid), using conservative remapping. To do this interpolation, we rely on the `cdo` library and use function `system` to invoke the OS command. Finally, sea points are removed by applying a land-sea mask. \n",
    "\n",
    "Therefore, we define two functions, `cdo.conservative.remap` and `mask.landsea`, which perform the interpolation and masking of the GCM outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T21:11:26.309149Z",
     "start_time": "2022-05-19T21:11:25.249Z"
    }
   },
   "outputs": [],
   "source": [
    "cdo.conservative.remap <- function(grid){\n",
    "  grid2nc(grid, NetCDFOutFile = \"./aux.nc4\")\n",
    "  system(\"cdo remapcon,data/mask.nc4 ./aux.nc4 ./aux2.nc4\")\n",
    "  rval <- loadGridData(\"./aux2.nc4\", var = predictand)\n",
    "  file.remove(c(\"./aux.nc4\",\"./aux2.nc4\"))\n",
    "  return(rval)\n",
    "}\n",
    "\n",
    "mask.landsea <- function(grid){\n",
    "  lapply(1:getShape(grid, \"time\"), function(t) {\n",
    "    gridArithmetics(\n",
    "      subsetDimension(grid, dimension = \"time\", indices = t),\n",
    "      mask\n",
    "    )\n",
    "  }) %>% bindGrid(dimension = \"time\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a loop over the temporal periods of interest (1975-2005 for the historical scenario plus 2006-2040, 2041-2070 and 2071-2100 for RCP8.5) and save the GCM ensemble as netCDF files (`grid2nc` function) in a multi-member C4R object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T21:11:26.309149Z",
     "start_time": "2022-05-19T21:11:25.249Z"
    }
   },
   "outputs": [],
   "source": [
    "for (period in names(periods)){\n",
    "  for (model.name in model.names) {\n",
    "    output.file <- sprintf(\"./data/%s/y_GCM_%s_%s.nc4\",\n",
    "      predictand,\n",
    "      period,\n",
    "      gsub(\"_.*\",\"\",model.name)\n",
    "    )\n",
    "    if (! file.exists(output.file)){\n",
    "      if (period == \"hist\"){\n",
    "        dataset.name <- sprintf(\"CMIP5-subset_%s_historical\", model.name)\n",
    "      } else {\n",
    "        dataset.name <- sprintf(\"CMIP5-subset_%s_rcp85\", model.name)\n",
    "      }\n",
    "      # Load the data and interpolate to the target resolution\n",
    "      y.gcm <- cdo.conservative.remap(loadGridData(\n",
    "          dataset = dataset.name,\n",
    "          var = predictand,\n",
    "          lonLim = c(-10,30), latLim = c(34,74),\n",
    "          years = periods[[period]]\n",
    "      ))\n",
    "      y.gcm <- mask.landsea(y.gcm)\n",
    "      grid2nc(y.gcm,NetCDFOutFile = output.file)\n",
    "    }\n",
    "  }  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Ensemble of Regional Climate Models (optional) \n",
    "In this section we form an ensemble of EURO-CORDEX RCMs which can be easily loaded from the Santander CDS by using the appropiate labels (see the block below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T21:12:10.962579Z",
     "start_time": "2022-05-19T21:12:10.952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Labels for the historical scenario\n",
    "rcm.labels.hist <- c(\n",
    "  \"CORDEX-EUR-44_CCCma-CanESM2_historical_r1i1p1_SMHI-RCA4_v1\",\n",
    "  \"CORDEX-EUR-44_CNRM-CERFACS-CNRM-CM5_historical_r1i1p1_ETH-CLMcom-CCLM5-0-6_v1\",\n",
    "  \"CORDEX-EUR-44_CNRM-CERFACS-CNRM-CM5_historical_r1i1p1_SMHI-RCA4_v1\",\n",
    "  \"CORDEX-EUR-44_MPI-M-MPI-ESM-LR_historical_r1i1p1_CLMcom-CCLM4-8-17_v1\",\n",
    "  \"CORDEX-EUR-44_MPI-M-MPI-ESM-LR_historical_r1i1p1_MPI-CSC-REMO2009_v1\",\n",
    "  \"CORDEX-EUR-44_NCC-NorESM1-M_historical_r1i1p1_SMHI-RCA4_v1\",\n",
    "  \"CORDEX-EUR-44_NOAA-GFDL-GFDL-ESM2M_historical_r1i1p1_SMHI-RCA4_v1\",\n",
    "  \"CORDEX-EUR-44_ICHEC-EC-EARTH_historical_r12i1p1_SMHI-RCA4_v1\",\n",
    "  \"CORDEX-EUR-44_ICHEC-EC-EARTH_historical_r12i1p1_ETH-CLMcom-CCLM5-0-6_v1\",\n",
    "  \"CORDEX-EUR-44_IPSL-IPSL-CM5A-MR_historical_r1i1p1_SMHI-RCA4_v1\",\n",
    "  \"CORDEX-EUR-44_IPSL-IPSL-CM5A-MR_historical_r1i1p1_IPSL-INERIS-WRF331F_v1\"\n",
    ")\n",
    "\n",
    "get.rcm.simulation.id <- function(x){\n",
    "  gcm <- unlist(strsplit(x,\"_\"))[2]\n",
    "  gcm <- unlist(strsplit(gcm,\"-\"))[-1]\n",
    "  if (gcm[[1]] %in% c(\"M\", \"CERFACS\", \"GFDL\")) # Drop modelling center remnants\n",
    "    gcm <- gcm[-1]\n",
    "  gcm <- paste(gcm, collapse=\"-\")\n",
    "  rcm <- unlist(strsplit(x,\"_\"))[5]\n",
    "  rcm <- unlist(strsplit(rcm,\"-\"))[-1]\n",
    "  if (rcm[[1]] %in% c(\"INERIS\", \"CLMcom\", \"CSC\")) # Drop modelling center remnants\n",
    "    rcm <- rcm[-1]\n",
    "  rcm <- paste(rcm, collapse=\"-\")\n",
    "  sprintf(\"%s_%s\", gcm, rcm)\n",
    "}\n",
    "\n",
    "# e.g.:\n",
    "get.rcm.simulation.id(rcm.labels.hist[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perfom a loop over the temporal periods of interest (1975-2005 for the historical scenario plus 2006-2040, 2041-2070 and 2071-2100 for RCP8.5) and save the RCM ensemble as netCDF files (`grid2nc` function) in a multi-member C4R object. All RCMs are interpolated to our target 0.5º resolution (E-OBS grid) and sea points are removed by applying the land-sea mask we have previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "periods.rcm <- list(\n",
    "  hist = 1975:2005, \n",
    "  nearf = 2006:2040,\n",
    "  midf = 2041:2070,\n",
    "  farf = 2071:2099\n",
    ")\n",
    "\n",
    "for (period in names(periods.rcm)){\n",
    "  for (dataset.name in rcm.labels.hist) {\n",
    "    output.file <- sprintf(\"./data/%s/y_RCM_%s_%s.nc4\",\n",
    "      predictand,\n",
    "      period,\n",
    "      get.rcm.simulation.id(dataset.name)\n",
    "    )\n",
    "    if (! file.exists(output.file)){\n",
    "      if (! period == \"hist\"){\n",
    "        dataset.name <- gsub(\"historical\",\"rcp85\", dataset.name)\n",
    "      }\n",
    "      # Load the data and interpolate to the target resolution\n",
    "      y.rcm <- loadGridData(\n",
    "          dataset = dataset.name,\n",
    "          var = predictand,\n",
    "          lonLim = c(-10,30), latLim = c(34,74),\n",
    "          years = periods.rcm[[period]]\n",
    "      ) %>% interpGrid(getGrid(mask))\n",
    "      y.rcm <- mask.landsea(y.rcm)  \n",
    "      grid2nc(y.rcm,NetCDFOutFile = output.file)\n",
    "    }\n",
    "  }  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides the code needed to reproduce the figures presented in the manuscript. Note we mostly rely on the `visualizeR` package for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Build multi-member grids (optional)\n",
    "***NOTE: Please skip this section if you work only with the Zenodo files and move directly to section 5.2***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 in the manuscript shows the climatology of the different ensembles built (GCM, RCM and DeepESD), along with the corresponding mean error (bias) with respect to the observed pattern in the historical period. We start thus by computing the climatology of the different contributing members forming each ensemble and saving them as netCDF files in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T21:12:30.621694Z",
     "start_time": "2022-05-19T21:12:30.600Z"
    }
   },
   "outputs": [],
   "source": [
    "# because a member of the RCM ensemble misses a value on 01-Jan-2006, so to preserve\n",
    "# temporal consistency in the ensemble we add this date to the ensemble mean metadata\n",
    "near.dates <- list(\n",
    "  start = \"2006-01-01 12:00:00 GMT\",\n",
    "    end = \"2041-01-01 12:00:00 GMT\"\n",
    ")\n",
    "model.types <- c(\"GCM\",\"RCM\",\"CNN\")\n",
    "gcm.basenames <- gsub(\"_.*\",\"\",model.names)\n",
    "rcm.basenames <- unlist(lapply(rcm.labels.hist, get.rcm.simulation.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (model.type in model.types){\n",
    "  basenames <- if (model.type == \"RCM\") {\n",
    "    rcm.basenames\n",
    "  } else {\n",
    "    gcm.basenames\n",
    "  }\n",
    "  for (period in names(periods)){\n",
    "    output.file <- sprintf(\"./data/%s/y_%s_%s_ensemble.nc4\", predictand, model.type, period)\n",
    "    if (! file.exists(output.file)){\n",
    "\n",
    "      ens.mean <- lapply(basenames, FUN = function(basename) {\n",
    "        path <- sprintf(\"./data/%s/y_%s_%s_%s.nc4\", predictand, model.type, period, basename)\n",
    "        grid <- loadGridData(dataset = path, var = predictand)\n",
    "        grid <- valueIndex(grid, index.code = \"Mean\")$Index  # compute mean of each member \n",
    "        if (period == \"nearf\") grid$Dates <- near.dates \n",
    "        return(grid)  \n",
    "      }) %>% bindGrid(dimension = \"member\")  # bind member means in a single C4R object along the `member` dimension    \n",
    "      ens.mean$InitializationDates <- NULL\n",
    "      grid2nc(ens.mean, NetCDFOutFile = output.file)\n",
    "        \n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ensemble mean and bias with respect to E-OBS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the mean climatology for each ensemble is obtained with the `aggregateGrid` function (note the aggregation is done along the member dimension) from `transformeR`. Afterwards, we can already use `spatialPlot` to plot the corresponding spatial pattern. The resulting figure is saved in `pdf` format in the path indicated in the `pdfOutput` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T20:44:05.553453Z",
     "start_time": "2022-05-19T20:44:05.520Z"
    }
   },
   "outputs": [],
   "source": [
    "if (predictand == \"pr\") {\n",
    "  palette <- \"BuPu\"\n",
    "  at <- seq(0,8,0.5)\n",
    "  units <- \"mm/day\"\n",
    "} else if (predictand == \"tas\") {\n",
    "  palette <- \"OrRd\"\n",
    "  at <- seq(-5, 20,2.5)\n",
    "  units <- \"ºC\"\n",
    "}\n",
    "\n",
    "cb <- c(\"#FFFFFF\",brewer.pal(n = 9, palette))\n",
    "cb <- cb %>% colorRampPalette()\n",
    "pdfOutput <- sprintf(\"./figures/ensembleMean_%s.pdf\" , predictand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T09:32:22.399749Z",
     "start_time": "2022-05-21T09:32:22.386Z"
    }
   },
   "outputs": [],
   "source": [
    "figs <- lapply(model.types, FUN = function(model.type) {\n",
    "  \n",
    "  # Compute the ensemble mean  \n",
    "  ensemble_climatology_hist <- loadGridData(sprintf(\"./data/%s/y_%s_hist_ensemble.nc4\", predictand, model.type), \n",
    "                                            var = \"Mean\") %>% \n",
    "      aggregateGrid(aggr.mem = list(FUN = \"mean\", na.rm = TRUE))\n",
    "  \n",
    "  # We depict the ensemble mean with spatialPlot function  \n",
    "  spatialPlot(\n",
    "    ensemble_climatology_hist,\n",
    "    backdrop.theme = \"coastline\",\n",
    "    main = sprintf(\"Ensemble Mean (%s) - %s\", units, model.type),\n",
    "    ylab = \"1975-2005\",\n",
    "    col.regions = cb,\n",
    "    at = at,\n",
    "    set.min = at[1], \n",
    "    set.max = at[length(at)])\n",
    "})\n",
    "\n",
    "pdf(pdfOutput, width = 15, height = 10)   \n",
    "grid.arrange(grobs = figs, ncol = 3)                     \n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the bias with respect to the observed (i.e. E-OBS) climatology. Again, we rely on `spatialPlot` to depict the spatial fields, and `aggregateGrid` and `gridArithmetics`, to compute the ensemble mean and its bias, respectively. The resulting figures are saved in `pdf` format in the path indicated by `pdfOutput`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T20:44:34.601831Z",
     "start_time": "2022-05-19T20:44:33.252Z"
    }
   },
   "outputs": [],
   "source": [
    "if (predictand == \"pr\") {\n",
    "  cb <- brewer.pal(n = 11, \"BrBG\")\n",
    "  cb[6] <- \"#FFFFFF\"; cb <- cb %>% colorRampPalette()\n",
    "  at <- c(seq(-2, -0.5,0.5),-0.25,0.25,seq(0.5, 2,0.5))    \n",
    "  units <- \"mm/day\"\n",
    "} else if (predictand == \"tas\") {\n",
    "  cb <- rev(brewer.pal(n = 11, \"RdBu\"))\n",
    "  cb[6] <- \"#FFFFFF\"; cb <- cb %>% colorRampPalette()\n",
    "  at <- c(seq(-2, -0.5,0.5),-0.25,0.25, seq(0.5,2,0.5)) \n",
    "  units <- \"ºC\"   \n",
    "}\n",
    "pdfOutput <- sprintf(\"./figures/bias_%s.pdf\", predictand) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T20:45:16.782016Z",
     "start_time": "2022-05-19T20:44:39.381Z"
    }
   },
   "outputs": [],
   "source": [
    "y_climatology <- valueIndex(y, index.code = \"Mean\")$Index\n",
    "figs <- lapply(model.types, FUN = function(model.type) {\n",
    "    \n",
    "  # Compute the ensemble mean  \n",
    "  ensemble_climatology_hist <- loadGridData(sprintf(\"./data/%s/y_%s_hist_ensemble.nc4\", predictand, model.type), \n",
    "                                            var = \"Mean\") %>% \n",
    "    aggregateGrid(aggr.mem = list(FUN = \"mean\", na.rm = TRUE))\n",
    "\n",
    "  # Compute the bias with respect to the observed temporal climatology for the same period\n",
    "  bias_hist <- gridArithmetics(ensemble_climatology_hist,\n",
    "                               y_climatology,\n",
    "                               operator = \"-\")\n",
    "    \n",
    "  # Depict the bias of the ensemble mean\n",
    "  spatialPlot(bias_hist,\n",
    "              backdrop.theme = \"coastline\",\n",
    "              main = sprintf(\"Bias Ensemble Mean (%s) - %s\", units, model.type),\n",
    "              ylab = \"1975-2005\",\n",
    "              col.regions = cb,\n",
    "              at = at,\n",
    "              set.min = at[1], \n",
    "              set.max = at[length(at)]\n",
    "             )  \n",
    "}) \n",
    "pdf(pdfOutput, width = 15, height = 10)   \n",
    "grid.arrange(grobs = figs, ncol = 3)                     \n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Climate change signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce Figure 2 in the manuscript we perform a loop, for each ensemble (GCM, RCM and DeepESD), over the different RCP8.5 periods of interest and sequentially compute the difference between the future climatology and the historical one. These climate change signals are then averaged along the member dimension using the `aggregateGrid` function. The resulting figures are saved in `pdf` format in the path indicated by `pdfOutput`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (predictand == \"pr\") {\n",
    "  cb <- brewer.pal(n = 11, \"BrBG\")\n",
    "  cb[6] <- \"#FFFFFF\"; cb <- cb %>% colorRampPalette()\n",
    "  at <- c(seq(-1, -0.25,0.25),-0.125,0.125,seq(0.25, 1,0.25)) \n",
    "} else if (predictand == \"tas\") {\n",
    "  cb <- c(\"#FFFFFF\",brewer.pal(n = 11, \"OrRd\"))\n",
    "  at <- c(seq(0,4,0.5),5,6)\n",
    "}\n",
    "pdfOutput <- sprintf(\"./figures/deltas_%s.pdf\", predictand) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs <- lapply(names(periods)[-1], FUN = function(period) {  \n",
    "    lapply(model.types, FUN = function(model.type) {\n",
    "    grid_hist <- loadGridData(sprintf(\"./data/%s/y_%s_hist_ensemble.nc4\", predictand, model.type), \n",
    "                              var = \"Mean\")\n",
    "    grid_future <- loadGridData(sprintf(\"./data/%s/y_%s_%s_ensemble.nc4\", predictand, model.type, period), \n",
    "                                var = \"Mean\") \n",
    "        \n",
    "    grid <- gridArithmetics(grid_future,\n",
    "                            grid_hist,\n",
    "                            operator = \"-\")\n",
    "        \n",
    "    ensemble_delta_mean <-  aggregateGrid(grid, aggr.mem = list(FUN = \"mean\", na.rm = TRUE)) \n",
    "\n",
    "    spatialPlot(ensemble_delta_mean,\n",
    "                backdrop.theme = \"coastline\",\n",
    "                main = \"CC. signal wrt 1975-2005\",\n",
    "                ylab = periods[[period]],\n",
    "                col.regions = cb,\n",
    "                at = at,\n",
    "                set.min = at[1], \n",
    "                set.max = at[length(at)]\n",
    "               )  \n",
    "  }) \n",
    "}) %>% unlist(recursive = FALSE)   \n",
    "pdf(pdfOutput, width = 15, height = 10)   \n",
    "grid.arrange(grobs = figs, ncol = 3)                     \n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code allows for plotting the time-series of the climate change signals. For precipitation (temperature), we perform a loop over the validation metrics of interest: R01, SDII (Mean). For details about these metrics please see the manuscript or type `show.indices()` in a new cell. At each iteration of the loop we define a `doCall.args` list which contains the `aggr.y` arguments needed for the `aggregateGrid` function (note the validation is done at an annual basis), which is finally passed t  `do.call`. At the end of the loop, the resulting figures are saved in `pdf` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (predictand == \"pr\") {\n",
    "  indices <- c(\"R01\",\"SDII\")\n",
    "} else if (predictand == \"tas\") {\n",
    "  indices <- \"Mean\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interannual_timeserie <- function(model.type, members, doCall.args) {\n",
    "  lapply(names(periods), FUN = function(period) { \n",
    "    lapply(members, FUN = function(member) {            \n",
    "      doCall.args[[\"grid\"]] <- loadGridData(dataset = sprintf(\"./data/%s/y_%s_%s_%s.nc4\", \n",
    "                                                              predictand, \n",
    "                                                              model.type, \n",
    "                                                              period, \n",
    "                                                              member),\n",
    "                                            var = predictand)\n",
    "      do.call(\"aggregateGrid\",doCall.args)\n",
    "    }) %>% bindGrid(dimension = \"member\")\n",
    "  }) %>% bindGrid(dimension = \"time\")     \n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs <- lapply(indices, FUN = function(zz) {\n",
    "  doCall.args <- list() \n",
    "  doCall.args[[\"aggr.y\"]] <- list()\n",
    "  \n",
    "  # The R01 do.call arguments  \n",
    "  if (zz == \"R01\")  {\n",
    "    doCall.args[[\"aggr.y\"]][[\"FUN\"]] <- \"index.freq\"\n",
    "    doCall.args[[\"aggr.y\"]][[\"freq.type\"]] <- \"rel\"\n",
    "    doCall.args[[\"aggr.y\"]][[\"condition\"]] <- \"GE\"\n",
    "    doCall.args[[\"aggr.y\"]][[\"threshold\"]] <- 1\n",
    "    ylim <- c(0.24,0.54)  \n",
    "  }\n",
    "  # The SDII do.call arguments  \n",
    "  if (zz == \"SDII\"){\n",
    "    doCall.args[[\"aggr.y\"]][[\"FUN\"]] <- \"index.meanGE\"\n",
    "    doCall.args[[\"aggr.y\"]][[\"threshold\"]] <- 1\n",
    "    ylim <- c(2,9)  \n",
    "  } \n",
    "  # The Mean do.call arguments    \n",
    "  if (zz == \"Mean\"){\n",
    "    doCall.args[[\"aggr.y\"]][[\"FUN\"]]   <- \"mean\"\n",
    "    doCall.args[[\"aggr.y\"]][[\"na.rm\"]] <- TRUE\n",
    "    ylim <- c(0,18)  \n",
    "  }    \n",
    "    \n",
    "  # We compute the index for the GCM ensemble. \n",
    "  gcm_timeserie <- interannual_timeserie(model.type = \"GCM\", \n",
    "                                         members = gcm.basenames, \n",
    "                                         doCall.args)\n",
    "  \n",
    "  # We compute the index for the RCM ensemble. \n",
    "  rcm_timeserie <- interannual_timeserie(model.type = \"RCM\", \n",
    "                                         members = rcm.basenames, \n",
    "                                         doCall.args)\n",
    " \n",
    "  # We compute the index for the RCM ensemble. \n",
    "  cnn_timeserie <- interannual_timeserie(model.type = \"CNN\", \n",
    "                                         members = gcm.basenames, \n",
    "                                         doCall.args)\n",
    "      \n",
    "  # We compute the index for the observed temporal serie \n",
    "  doCall.args[[\"grid\"]] <- y\n",
    "  obs_timeserie <- do.call(\"aggregateGrid\",doCall.args)\n",
    "  \n",
    "  # We call temporalPlot to plot the times-series \n",
    "  temporalPlot(\"OBS\" = obs_timeserie, \n",
    "               \"GCM\" = gcm_timeserie,\n",
    "               \"RCM\" = rcm_timeserie,\n",
    "               \"CNN\" = cnn_timeserie, \n",
    "               cols = c(\"black\",\"red\",\"blue\",\"green\"),\n",
    "               xyplot.custom = list(ylim = ylim))       \n",
    "})\n",
    "\n",
    "# Saving the resulting figures in .pdf format\n",
    "pdf(sprintf(\"./figures/serie_%s.pdf\", predictand), width = 15, height = 4)\n",
    "grid.arrange(grobs = figs, ncol = 1)  \n",
    "dev.off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "386px",
    "width": "400px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
